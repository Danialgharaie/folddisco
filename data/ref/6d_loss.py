# -*- coding: utf-8 -*-
"""6D_loss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16xQETxroVuLdt8bhZUT-cqCLGT8IVcri
"""

import matplotlib.pyplot as plt
import numpy as np

"""#PYTORCH"""

import torch

def th_norm(x,eps:float=1e-8):
  return x.square().sum(-1,keepdim=True).add(eps).sqrt()

def th_N(x,alpha:float=0):
  return x/th_norm(x).add(alpha)

def th_cross(a,b):
  a,b = torch.broadcast_tensors(a,b)
  return torch.cross(a,b)

def th_len(a,b, log:bool=False):
  dist = th_norm(a-b)
  return torch.log(dist+1) if log else dist

def th_ang(a,b,c, acos:bool=False):
  angle = (th_N(b-a)*th_N(b-c)).sum(-1,keepdim=True)
  if acos: return torch.acos(angle)
  else: return angle

def th_dih(a,b,c,d, atan2:bool=False, norm:bool=True):
  bc = th_N(b-c)
  n1 = th_cross(th_N(a-b),bc)
  n2 = th_cross(bc,th_N(c-d))
  sin_angle = (th_cross(n1,bc)*n2).sum(-1)
  cos_angle = (n1*n2).sum(-1)
  if atan2: return torch.atan2(sin_angle,cos_angle)
  else:
    dih = torch.stack((sin_angle,cos_angle),-1)
    return th_N(dih) if norm else dih

def th_get_CB(N,CA,C):
  b,c = CA-N,C-CA
  a = th_cross(b,c)
  return CA - 0.58273431*a + 0.56802827*b - 0.54067466*c

def th_get_6D(coords, norm_dih:bool=False, log_dist:bool=True):
  N,CA,C = coords.unbind(-2)
  CB = th_get_CB(N,CA,C)
  CB_i,CB_j = CB.unsqueeze(-2),CB.unsqueeze(-3)
  CA_i,CA_j = CA.unsqueeze(-2),CA.unsqueeze(-3)
  N__i = N.unsqueeze(-2)
  feat = {"dist":  th_len(CB_i,CB_j,log=log_dist),
          "omega": th_dih(CA_i,CB_i,CB_j,CA_j,norm=norm_dih),
          "theta": th_dih(N__i,CA_i,CB_i,CB_j,norm=norm_dih),
          "phi":   th_ang(CA_i,CB_i,CB_j)} 
  return feat

def th_loss_6D(true, pred, norm_dih:bool=False, log_dist:bool=True):
  t = th_get_6D(true,norm_dih,log_dist)
  p = th_get_6D(pred,norm_dih,log_dist)
  w = {"dist":1/6,"omega":1/6,"theta":2/6,"phi":2/6}
  loss = [w[k]*torch.square(t[k]-p[k]).sum(-1) for k in t.keys()]
  return torch.stack(loss,0).sum(0).mean((-1,-2))

# optimize (a slight improvement in speed)
th_loss_6D_jit = torch.jit.script(th_loss_6D)

###########
# TODO: make TorchScript friendly
###########
def th_get_BB(coords, norm_dih:bool=False, log_dist:bool=True):
  coords = coords.flatten(-3,-2)
  def X(a=0,b=None): return coords[...,a:b,:]
  return {"dist": th_len(X(0,-1),X(1),log=log_dist),
          "ang":  th_ang(X(0,-2),X(1,-1),X(2)),
          "dih":  th_dih(X(0,-3),X(1,-2),X(2,-1),X(3),norm=norm_dih)}

def th_loss_BB(true, pred, norm_dih:bool=False, log_dist:bool=True):
  # add loss on the backbone bond-length, angle and dihedral.
  t = th_get_BB(true,norm_dih,log_dist)
  p = th_get_BB(pred,norm_dih,log_dist)
  w = {"dist":1/3,"ang":1/3,"dih":1/3}
  return torch.stack([w[k]*torch.square(t[k]-p[k]).sum(-1).mean(-1) for k in t.keys()],0).sum(0)

"""#TENSORFLOW"""

import tensorflow as tf

def tf_norm(x, axis=-1, keepdims=True, eps=1e-8):
  return tf.sqrt(tf.reduce_sum(tf.square(x),axis=axis, keepdims=keepdims) + eps)

def tf_cross(x,y):
  shape = tf.maximum(tf.shape(x),tf.shape(y))
  x = tf.broadcast_to(x,shape)
  y = tf.broadcast_to(y,shape)
  return tf.linalg.cross(x,y)

def tf_flatten(x, start=0, end=-1):
  shape, ndim = tf.shape(x),tf.rank(x)
  start, end = start % ndim, end % ndim
  shape = tf.concat([shape[:start],[-1],shape[(end+1):]],-1)
  return tf.reshape(x,shape)

def tf_len(a,b, log=False):
  dist = tf_norm(a-b)
  return tf.math.log(dist+1) if log else dist

def tf_ang(a,b,c, acos=False):
  N = lambda x: x/tf_norm(x)
  angle = tf.reduce_sum(N(b-a)*N(b-c),-1,keepdims=True)
  if acos: return tf.acos(angle)
  else: return angle

def tf_dih(a,b,c,d, atan2=False, norm=True):
  N = lambda x: x/tf_norm(x)
  bc = N(b-c)
  n1 = tf_cross(N(a-b),bc)
  n2 = tf_cross(bc,N(c-d))
  sin_angle = tf.reduce_sum(tf_cross(n1,bc)*n2,-1)
  cos_angle = tf.reduce_sum(n1*n2,-1)
  if atan2: return tf.atan2(sin_angle,cos_angle)
  else:
    dih = tf.stack((sin_angle,cos_angle),-1)
    return N(dih) if norm else dih

def tf_extend(a,b,c, L,A,D):
  N = lambda x: x/tf_norm(x)
  bc = N(b-c)
  n = N(tf_cross(b-a, bc))
  m = [bc,tf_cross(n,bc),n]
  d = [L*tf.cos(A), L*tf.sin(A)*tf.cos(D), -L*tf.sin(A)*tf.sin(D)]
  return c + sum([m*d for m,d in zip(m,d)])

def tf_get_CB(N,CA,C):
  b,c = CA-N,C-CA
  a = tf_cross(b,c)
  return CA - 0.58273431*a + 0.56802827*b - 0.54067466*c

def tf_get_6D(coords, norm_dih=False, log_dist=True, concat=False):
  N,CA,C = [coords[...,k,:] for k in range(3)]
  CB = tf_get_CB(N,CA,C) # tf_extend(C,N,CA,1.522,1.927,-2.143)
  # (,L,3) -> (,L,None,3),(,None,L,3)
  CB_i,CB_j = tf.expand_dims(CB,-2),tf.expand_dims(CB,-3)
  CA_i,CA_j = tf.expand_dims(CA,-2),tf.expand_dims(CA,-3)
  N__i = tf.expand_dims(N,-2)
  out = {"dist":  tf_len(CB_i,CB_j,log=log_dist),
         "omega": tf_dih(CA_i,CB_i,CB_j,CA_j,norm=norm_dih),
         "theta": tf_dih(N__i,CA_i,CB_i,CB_j,norm=norm_dih),
         "phi":   tf_ang(CA_i,CB_i,CB_j)} 
  if concat: return tf.concat([out[k] for k in ["dist","omega","theta","phi"]],-1)
  else: return out

def tf_loss_6D(true, pred, dist_cutoff=None, norm_dih=False, log_dist=True):
  t = tf_get_6D(true,norm_dih,log_dist)
  p = tf_get_6D(pred,norm_dih,log_dist)
  w = {"dist":1/6,"omega":1/6,"theta":2/6,"phi":2/6}
  sq_error = sum([w[k]*tf.reduce_sum(tf.square(t[k]-p[k]),-1) for k in t.keys()])
  if dist_cutoff is None:
    return tf.reduce_mean(sq_error,(-1,-2))
  else:
    if log_dist: dist_cutoff = np.log(dist_cutoff+1.0)
    mask = tf.cast(t["dist"][...,0] < dist_cutoff, dtype=tf.float32)
    return tf.reduce_sum(sq_error*mask,(-1,-2))/tf.reduce_sum(mask,(-1,-2))

# optimize
tf_loss_6D_fun = tf.function(tf_loss_6D, experimental_relax_shapes=True)
tf_loss_6D_jit = tf.function(tf_loss_6D, experimental_compile=True)

def tf_get_BB(coords, norm_dih=False, log_dist=True):
  # (,L,3,3)->(,L*3,3)
  coords = tf_flatten(coords,-3,-2)
  def X(a=0,b=None): return coords[...,a:b,:]
  return {"dist": tf_len(X(0,-1),X(1),log=log_dist),
          "ang":  tf_ang(X(0,-2),X(1,-1),X(2)),
          "dih":  tf_dih(X(0,-3),X(1,-2),X(2,-1),X(3),norm=norm_dih)}

def tf_loss_BB(true, pred, norm_dih=False, log_dist=True):
  # add loss on the backbone bond-length, angle and dihedral.
  t = tf_get_BB(true,norm_dih,log_dist)
  p = tf_get_BB(pred,norm_dih,log_dist)
  w = {"dist":1/3,"ang":1/3,"dih":1/3}
  return sum([w[k]*tf.reduce_mean(tf.reduce_sum(tf.square(t[k]-p[k]),-1),-1) for k in t.keys()])

"""#NUMPY"""

def np_extend(a,b,c, L,A,D):
  N = lambda x: x/np.sqrt(np.square(x).sum(-1,keepdims=True) + 1e-8)
  bc = N(b-c)
  n = N(np.cross(b-a, bc))
  m = [bc,np.cross(n,bc),n]
  d = [L*np.cos(A), L*np.sin(A)*np.cos(D), -L*np.sin(A)*np.sin(D)]
  return c + sum([m*d for m,d in zip(m,d)])

def np_get_CB(N,CA,C):
  b,c = CA-N,C-CA
  a = np.cross(b,c)
  return CA - 0.58273431*a + 0.56802827*b - 0.54067466*c

def np_len(a,b):
  '''given coordinates a-b, return length or distance'''
  return np.sqrt(np.sum(np.square(a-b),axis=-1))

def np_ang(a,b,c, acos=False):
  '''given coordinates a-b-c, return angle'''
  D = lambda x,y: np.sum(x*y,axis=-1)
  N = lambda x: x/np.sqrt(np.square(x).sum(-1,keepdims=True) + 1e-8)
  ang = D(N(b-a),N(b-c))
  if acos: return np.arccos(ang)
  else: return ang

def np_dih(a,b,c,d, atan2=False, norm=False, eps1=1e-8,eps2=0):
  '''given coordinates a-b-c-d, return dihedral'''
  D = lambda x,y: np.sum(x*y,axis=-1)
  N = lambda x: x/(np.sqrt(np.square(x).sum(-1,keepdims=True)+eps1)+eps2)
  bc = N(b-c)
  n1 = np.cross(N(a-b),bc)
  n2 = np.cross(bc,N(c-d))
  sin_angle = D(np.cross(n1,bc),n2)
  cos_angle = D(n1,n2)
  if atan2: return np.arctan2(sin_angle,cos_angle)
  else:
    dih = np.stack([sin_angle,cos_angle],-1)
    return N(dih) if norm else dih

def np_get_6D(coords):
  N,CA,C = coords[:,0], coords[:,1], coords[:,2]
  #CB = np_extend(C, N, CA, 1.522, 1.927, -2.143)
  CB = np_get_CB(N,CA,C)
  out = {"dist":  np_len(CB[:,None], CB[None,:]),
         "omega": np_dih(CA[:,None], CB[:,None], CB[None,:], CA[None,:]),
         "theta": np_dih( N[:,None], CA[:,None], CB[:,None], CB[None,:]),
         "phi":   np_ang(CA[:,None], CB[:,None], CB[None,:])}
  return out

def parse_PDB(x, atoms=['N','CA','C'], chain=None):
  xyz,seq,min_resn,max_resn = {},{},np.inf,-np.inf
  for line in open(x,"rb"):
    line = line.decode("utf-8","ignore").rstrip()
    if line[:6] == "HETATM" and line[17:17+3] == "MSE":
      line = line.replace("HETATM","ATOM  ")
      line = line.replace("MSE","MET")
    if line[:4] == "ATOM":
      ch = line[21:22]
      if ch == chain or chain is None:
        atom = line[12:12+4].strip()
        resi = line[17:17+3]
        resn = line[22:22+5].strip()
        x,y,z = [float(line[i:(i+8)]) for i in [30,38,46]]
        if resn[-1].isalpha(): resa,resn = resn[-1],int(resn[:-1])-1
        else: resa,resn = "",int(resn)-1
        if resn < min_resn: min_resn = resn
        if resn > max_resn: max_resn = resn
        if resn not in xyz: xyz[resn] = {}
        if resa not in xyz[resn]: xyz[resn][resa] = {}
        if resn not in seq: seq[resn] = {}
        if resa not in seq[resn]: seq[resn][resa] = resi
        if atom not in xyz[resn][resa]:
          xyz[resn][resa][atom] = np.array([x,y,z])

  # convert to numpy arrays, fill in missing values
  seq_,xyz_ = [],[]
  for resn in range(min_resn,max_resn+1):
    if resn in seq:
      for k in sorted(seq[resn]): seq_.append(seq[resn][k])
    else: seq_.append("UNK")
    if resn in xyz:
      for k in sorted(xyz[resn]):
        for atom in atoms:
          if atom in xyz[resn][k]: xyz_.append(xyz[resn][k][atom])
          else: xyz_.append(np.full(3,np.nan))
    else:
      for atom in atoms: xyz_.append(np.full(3,np.nan))
      
  return np.array(xyz_).reshape(-1,len(atoms),3), np.array(seq_)

"""#TEST - testing if all implementations give same result"""

!wget -qnc https://files.ipd.uw.edu/krypton/TrRosetta/design/6MRR.pdb

# numpy
coords, seq = parse_PDB("6MRR.pdb")
coords_noise = coords + np.random.normal(0,3,size=coords.shape)
np_6D = np_get_6D(coords)

"""###pytorch"""

###############################################################
# pytorch
###############################################################
coords_th_ = torch.tensor(coords,device=torch.device("cuda:0"))
coords_noise_th_ = torch.tensor(coords_noise,device=torch.device("cuda:0"))
th_6D = th_get_6D(coords_th_, log_dist=False, norm_dih=False)
th_loss = th_loss_6D(coords_th_, coords_noise_th_).cpu().numpy()
###############################################################
plt.figure(figsize=(7,7))
for n,key in enumerate(np_6D.keys()):
  plt.subplot(2,2,n+1)
  plt.title(key)
  plt.scatter(np_6D[key],th_6D[key].cpu().numpy(),s=1)
plt.show()

"""###tensorflow"""

###############################################################
# tensorflow
###############################################################
coords_tf_ = tf.constant(coords,dtype=tf.float32)
coords_noise_tf_ = tf.constant(coords_noise,dtype=tf.float32)
tf_6D = tf_get_6D(coords_tf_, log_dist=False)
tf_loss = tf_loss_6D(coords_tf_, coords_noise_tf_).numpy()
###############################################################
plt.figure(figsize=(7,7))
for n,key in enumerate(np_6D.keys()):
  plt.subplot(2,2,n+1)
  plt.title(key)
  plt.scatter(np_6D[key],tf_6D[key].numpy(),s=1)
plt.show()

print(th_loss,tf_loss)

"""# speed test (evaluate)"""

def gen_random_xyz(ln=None,tensor=False):
  if ln is None:
    ln = np.random.randint(100,150)
  coords_a = np.random.normal(size=(ln,3,3)).astype(np.float32)
  coords_b = np.random.normal(size=(ln,3,3)).astype(np.float32)
  if tensor:
    coords_a = torch.tensor(coords_a,device=torch.device("cuda:0"),dtype=torch.float32)
    coords_b = torch.tensor(coords_b,device=torch.device("cuda:0"),dtype=torch.float32)
  return coords_a,coords_b

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for _ in range(1000):
#   _ = th_loss_6D(*gen_random_xyz(tensor=True))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for _ in range(1000):
#   _ = th_loss_6D_jit(*gen_random_xyz(tensor=True))

"""###tensorflow"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for _ in range(1000):
#   _ = tf_loss_6D(*gen_random_xyz())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for _ in range(1000):
#   _ = tf_loss_6D_fun(*gen_random_xyz())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for _ in range(1000):
#   _ = tf_loss_6D_jit(*gen_random_xyz())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # SUPER FAST assuming same length input
# for _ in range(1000):
#   _ = tf_loss_6D_jit(*gen_random_xyz(ln=100))

"""#speed test (optimize)"""

def coord_rms_loss(P, Q, ca_only:bool=True):
  def rmsd(V, W): return (V-W).square().sum(-1).mean(-1).add(1e-8).sqrt()
  def centroid(X): return X.mean(-2,keepdims=True)
  if ca_only:
    P,Q = P[...,1,:],Q[...,1,:]
  P = P - centroid(P)
  Q = Q - centroid(Q)
  C = torch.matmul(P.transpose(-1,-2), Q)
  V, S, W = torch.svd(C)
  d = torch.ones_like(W)
  d[...,-1] = torch.sign(torch.det(V) * torch.det(W))[...,None]
  U = torch.matmul(d*V, W.transpose(-1,-2))
  rP = torch.matmul(P, U)
  return rmsd(rP, Q)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# losses = []
# coords_a = torch.tensor(coords,device=torch.device("cuda:0"),dtype=torch.float32)
# coords_b = torch.tensor(coords_noise,device=torch.device("cuda:0"),dtype=torch.float32)
# coords_c = torch.autograd.Variable(coords_b,requires_grad=True)
# optimizer = torch.optim.Adam([coords_c],lr=1e-1)
# for k in range(1000):
#   optimizer.zero_grad()
#   loss_th_ = th_loss_6D(coords_a,coords_c).mean()
#   losses.append(float(loss_th_.detach().cpu().numpy()))
#   loss_th_.backward()
#   optimizer.step()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# losses = []
# coords_a = torch.tensor(coords,device=torch.device("cuda:0"),dtype=torch.float32)
# coords_b = torch.tensor(coords_noise,device=torch.device("cuda:0"),dtype=torch.float32)
# coords_c = torch.autograd.Variable(coords_b,requires_grad=True)
# optimizer = torch.optim.Adam([coords_c],lr=1e-1)
# for k in range(1000):
#   optimizer.zero_grad()
#   loss_th_ = th_loss_6D_jit(coords_a,coords_c).mean()
#   losses.append(float(loss_th_.detach().cpu().numpy()))
#   loss_th_.backward()
#   optimizer.step()

plt.figure(figsize=(5,5))
plt.plot(losses)
plt.xlabel("iterations")

"""###tensorflow"""

from tensorflow.keras.optimizers import Adam
x_true = tf.constant(coords,dtype=tf.float32)
x = tf.Variable(coords_noise,dtype=tf.float32)
opt = Adam(1e-1)

@tf.function()
def step():
  with tf.GradientTape() as tape:
    loss_ = tf_loss_6D(x_true,x)
  x_grad = tape.gradient(loss_,[x])
  opt.apply_gradients(grads_and_vars=zip(x_grad,[x]))
  return loss_
losses_tf = [step().numpy()]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for k in range(10000):
#   losses_tf.append(step().numpy())

from tensorflow.keras.optimizers import Adam
x_true = tf.constant(coords,dtype=tf.float32)
x = tf.Variable(coords_noise,dtype=tf.float32)
opt = Adam(1e-1)

@tf.function(experimental_compile=True)
def step():
  with tf.GradientTape() as tape:
    loss_ = tf_loss_6D(x_true,x)
  x_grad = tape.gradient(loss_,[x])
  opt.apply_gradients(grads_and_vars=zip(x_grad,[x]))
  return loss_
losses_tf = [step().numpy()]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for k in range(10000):
#   losses_tf.append(step().numpy())

"""###opt_test

#tensorflow graph_mode
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # tensorflow graph mode
# tf.compat.v1.disable_eager_execution()
# tf.compat.v1.reset_default_graph()
# coords_a_ = tf.compat.v1.placeholder(dtype=tf.float32,shape=[None,3,3])
# coords_b_ = tf.compat.v1.placeholder(dtype=tf.float32,shape=[None,3,3])
# loss_tf_ = tf_loss_6D(coords_a_, coords_b_)
# sess = tf.compat.v1.Session()

coords_a,coords_b = gen_random_xyz()
_ = sess.run(loss_tf_,{coords_a_:coords_a,
                       coords_b_:coords_b})

coords_a.shape

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for _ in range(1000):
#   coords_a,coords_b = gen_random_xyz()
#   _ = sess.run(loss_tf_,{coords_a_:coords_a,coords_b_:coords_b})



"""#TEST"""

def coord_rms_loss(P, Q):
  # (B,L,3), (B,L,3)
  ##############################
  # WARNING NOT BATCH-FRIENDLY!!
  # TODO: make batch-friendly
  ##############################
  P,Q = P[0],Q[0]
  def rmsd(V, W): return torch.sqrt( torch.sum( (V-W)*(V-W) ) / len(V) + 1e-8)
  def centroid(X): return X.mean(axis=0)
  P = P - centroid(P)
  Q = Q - centroid(Q)
  C = torch.mm(P.T, Q)
  V, S, W = torch.svd(C)
  d = torch.ones([3,3],device=P.device)
  d[:,-1] = torch.sign(torch.det(V) * torch.det(W))
  U = torch.mm(d*V, W.T)
  rP = torch.mm(P, U)
  rms = rmsd(rP, Q)
  return rms[None]

torch.matmul(torch.tensor(np.ones((100,10,3))),
             torch.tensor(np.ones((100,10,3))).transpose(-1,-2)).shape



def coord_rms_loss(P, Q):
  # (B,L,3), (B,L,3)
  ##############################
  # WARNING NOT BATCH-FRIENDLY!!
  # TODO: make batch-friendly
  ##############################
  def rmsd(V, W): return torch.sqrt( torch.sum( (V-W)*(V-W) ) / len(V) + 1e-8)
  def centroid(X): return X.mean(axis=0)
  P = P - centroid(P)
  Q = Q - centroid(Q)
  C = torch.mm(P.T, Q)
  V, S, W = torch.svd(C)
  d = torch.ones([3,3],device=P.device)
  d[:,-1] = torch.sign(torch.det(V) * torch.det(W))
  U = torch.mm(d*V, W.T)
  rP = torch.mm(P, U)
  rms = rmsd(rP, Q)
  return rms[None]

a = torch.tensor(np.random.normal(size=(10,3)))
b = torch.tensor(np.random.normal(size=(10,3)))
print(coord_rms_loss(a,a),coord_rms_loss_(a,a))

a = np.array([[1,-1,0],
              [0,0,0],
              [1,0,0],
              [0,1,0]])

plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.plot(a[:,0],a[:,1])
plt.axis('equal')
plt.subplot(1,2,2)
plt.gcf().gca().add_artist(plt.Circle((0,0), 1, fill=False))
plt.scatter(*np_dih(*a,eps=0.1,norm=True))
plt.xlim(-1.1,1.1);plt.ylim(-1.1,1.1)

a = np.random.normal(0,2,size=(10000,4,3))

a[...,-1] = 0
a[...,0,:] = [-0.5,-1,0]
a[...,1,:] = [-0.5,0,0]
a[...,2,:] = [0.5,0,0]

plt.plot(a[0,:,0],a[0,:,1])

plt.scatter(*np_dih(*a.transpose((1,0,2))).T)

eps1 = 1e-8
eps2 = 1.0
plt.scatter(np_dih(*a.transpose((1,0,2)),norm=False,eps2=eps2)[:,1],np_dih(*a.transpose((1,0,2)),norm=True,eps2=eps2)[:,1],s=1)

